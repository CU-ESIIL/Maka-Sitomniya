{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datacube Exploration\n",
    "\n",
    "This notebook demonstrates how to combine multiple CMIP datasets into a unified datacube and explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the scripts directory to the path so we can import our modules\n",
    "sys.path.append('../scripts')\n",
    "from cmip_processor import CMIPProcessor, AggregationMethod\n",
    "from datacube_builder import DatacubeBuilder, InterpolationMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CMIP Datasets\n",
    "\n",
    "First, let's load the available CMIP datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Paths to the CMIP data files\ndata_files = {\n    \"rcp45\": \"./data/macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_monthly.nc\",\n    \"rcp85\": \"./data/macav2metdata_huss_CCSM4_r6i1p1_rcp85_2021_2025_CONUS_monthly.nc\"\n}\n\n# Import configuration if available\ntry:\n    # Add parent directory to path to import config\n    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n    import config\n    # Override paths with config paths if available\n    if hasattr(config, 'CMIP_CONFIG') and hasattr(config, 'DATA_DIR'):\n        model = config.CMIP_CONFIG.get(\"models\", [\"CCSM4\"])[0]\n        for scenario in config.CMIP_CONFIG.get(\"scenarios\", [\"rcp45\", \"rcp85\"]):\n            for variable in config.CMIP_CONFIG.get(\"variables\", [\"huss\"]):\n                years = config.CMIP_CONFIG.get(\"years\", \"2021_2025\")\n                region = config.CMIP_CONFIG.get(\"region\", \"CONUS\")\n                frequency = config.CMIP_CONFIG.get(\"frequency\", \"monthly\")\n                file_name = f\"macav2metdata_{variable}_{model}_r6i1p1_{scenario}_{years}_{region}_{frequency}.nc\"\n                file_path = os.path.join(config.DATA_DIR, file_name)\n                if scenario in data_files:\n                    data_files[scenario] = file_path\nexcept ImportError:\n    print(\"Configuration module not found. Using default paths.\")\n\n# Check for missing data files\navailable_files = {}\nmissing_files = []\nfor scenario, file_path in data_files.items():\n    if os.path.exists(file_path):\n        available_files[scenario] = file_path\n    else:\n        missing_files.append(file_path)\n\nif not available_files:\n    raise FileNotFoundError(\n        f\"No CMIP data files found. Please place at least one of the following files in the data directory:\\n\"\n        f\"- {', '.join(missing_files)}\\n\"\n        f\"or update the file paths to point to your data files.\"\n    )\nelif missing_files:\n    print(f\"Warning: Some data files are missing: {', '.join(missing_files)}\")\n    print(f\"Proceeding with available files: {', '.join(available_files.keys())}\")\n\n# Helper function to load datasets with non-standard calendars\ndef load_dataset_with_calendar_handling(file_path):\n    try:\n        # Try the standard CMIPProcessor\n        processor = CMIPProcessor(file_path)\n        return processor\n    except ValueError as e:\n        if \"unable to decode time units\" in str(e) or \"calendar\" in str(e):\n            print(f\"Warning: Non-standard calendar detected in {file_path}\")\n            print(\"Attempting to fix by manually loading the dataset...\")\n            \n            # Override CMIPProcessor._load_dataset method to handle non-standard calendars\n            import xarray as xr\n            \n            try:\n                # Try to import cftime\n                import cftime\n                print(\"Using cftime for non-standard calendar.\")\n                dataset = xr.open_dataset(file_path, use_cftime=True)\n            except ImportError:\n                print(\"cftime not available. Loading without decoding times.\")\n                dataset = xr.open_dataset(file_path, decode_times=False)\n            \n            # Create a processor and manually set the dataset\n            processor = CMIPProcessor(file_path)\n            processor.dataset = dataset\n            return processor\n        else:\n            # Re-raise if it's a different ValueError\n            raise\n\n# Initialize processors for each dataset\nprocessors = {}\nfor scenario, file_path in available_files.items():\n    try:\n        processors[scenario] = load_dataset_with_calendar_handling(file_path)\n        print(f\"Loaded {scenario} dataset with dimensions: {processors[scenario].dataset.dims}\")\n    except Exception as e:\n        print(f\"Error loading {scenario} dataset: {e}\")\n        print(f\"Skipping {scenario} dataset\")\n\nif not processors:\n    raise RuntimeError(\"Failed to load any datasets. Please ensure at least one valid CMIP data file is available.\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Datasets\n",
    "\n",
    "Let's compare basic statistics between the datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Compare statistics if we have processors\nif not processors:\n    print(\"No processors available for comparison\")\nelse:\n    # Get a list of all variables across all processors\n    all_vars = set()\n    for processor in processors.values():\n        all_vars.update(processor.dataset.data_vars)\n    \n    if not all_vars:\n        print(\"No variables found in datasets\")\n    else:\n        # Use the first variable found in the first processor\n        var_name = list(all_vars)[0]\n        \n        # Compare statistics\n        stats = []\n        for scenario, processor in processors.items():\n            if var_name in processor.dataset.data_vars:\n                var = processor.dataset[var_name]\n                stats.append({\n                    \"Scenario\": scenario,\n                    \"Min\": float(var.min()),\n                    \"Max\": float(var.max()),\n                    \"Mean\": float(var.mean()),\n                    \"Std\": float(var.std())\n                })\n        \n        # Convert to DataFrame for easy viewing\n        if stats:\n            stats_df = pd.DataFrame(stats)\n            display(stats_df)\n        else:\n            print(f\"Variable {var_name} not found in any processor's dataset\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Differences Between Scenarios\n",
    "\n",
    "Let's create maps to visualize the differences between the RCP4.5 and RCP8.5 scenarios."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check if we have both RCP4.5 and RCP8.5 scenarios\nif \"rcp45\" in processors and \"rcp85\" in processors:\n    # Get common variable name\n    common_vars = set(processors[\"rcp45\"].dataset.data_vars).intersection(\n        set(processors[\"rcp85\"].dataset.data_vars)\n    )\n    \n    if not common_vars:\n        print(\"No common variables found between the scenarios\")\n    else:\n        var_name = list(common_vars)[0]\n        \n        # Calculate difference between scenarios for the first time point\n        rcp45_data = processors[\"rcp45\"].dataset[var_name].isel(time=0)\n        rcp85_data = processors[\"rcp85\"].dataset[var_name].isel(time=0)\n\n        # Check coordinate systems\n        print(\"RCP45 coordinates:\", rcp45_data.lat.shape, rcp45_data.lon.shape)\n        print(\"RCP85 coordinates:\", rcp85_data.lat.shape, rcp85_data.lon.shape)\n\n        # Ensure they have the same coordinates\n        if not (rcp45_data.lat.equals(rcp85_data.lat) and rcp45_data.lon.equals(rcp85_data.lon)):\n            print(\"Warning: Datasets have different coordinates.\")\n            \n            # If dimensions are identical but values differ\n            if rcp45_data.lat.shape == rcp85_data.lat.shape and rcp45_data.lon.shape == rcp85_data.lon.shape:\n                print(\"Dimensions match but values differ. Using as-is.\")\n                diff_data = rcp85_data - rcp45_data\n            else:\n                # Check which has the coarser resolution\n                if len(rcp45_data.lat) <= len(rcp85_data.lat) and len(rcp45_data.lon) <= len(rcp85_data.lon):\n                    # RCP45 is coarser, so downsample RCP85\n                    print(\"RCP45 has coarser resolution. Downsampling RCP85...\")\n                    rcp85_data = rcp85_data.sel(\n                        lat=rcp45_data.lat.values,\n                        lon=rcp45_data.lon.values,\n                        method=\"nearest\"\n                    )\n                else:\n                    # RCP85 is coarser, so downsample RCP45\n                    print(\"RCP85 has coarser resolution. Downsampling RCP45...\")\n                    rcp45_data = rcp45_data.sel(\n                        lat=rcp85_data.lat.values,\n                        lon=rcp85_data.lon.values,\n                        method=\"nearest\"\n                    )\n                \n                # Calculate difference\n                diff_data = rcp85_data - rcp45_data\n        else:\n            # Calculate difference if coordinates are already the same\n            diff_data = rcp85_data - rcp45_data\n\n        # Create a 1x3 subplot for comparison\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n        # Plot RCP4.5\n        rcp45_plot = rcp45_data.plot(ax=axes[0], cmap='viridis')\n        axes[0].set_title(f\"RCP4.5 {var_name} (First Month)\")\n\n        # Plot RCP8.5\n        rcp85_plot = rcp85_data.plot(ax=axes[1], cmap='viridis')\n        axes[1].set_title(f\"RCP8.5 {var_name} (First Month)\")\n\n        # Plot Difference\n        diff_plot = diff_data.plot(ax=axes[2], cmap='RdBu_r')\n        axes[2].set_title(f\"Difference (RCP8.5 - RCP4.5)\")\n\n        plt.tight_layout()\n        plt.show()\nelse:\n    # Get all available scenarios\n    available_scenarios = list(processors.keys())\n    \n    if not available_scenarios:\n        print(\"No scenarios available for visualization\")\n    else:\n        # Use the first scenario\n        scenario = available_scenarios[0]\n        var_names = list(processors[scenario].dataset.data_vars)\n        \n        if not var_names:\n            print(f\"No variables found in {scenario} dataset\")\n        else:\n            var_name = var_names[0]\n            \n            # Plot just the single available scenario\n            fig, ax = plt.subplots(figsize=(12, 8))\n            data = processors[scenario].dataset[var_name].isel(time=0)\n            data.plot(ax=ax, cmap='viridis')\n            ax.set_title(f\"{scenario.upper()} {var_name} (First Month)\")\n            plt.tight_layout()\n            plt.show()\n            \n            print(f\"Note: Only {scenario} scenario is available. Cannot calculate differences between scenarios.\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Datasets for Datacube Integration\n",
    "\n",
    "Now let's process each dataset to prepare for integration into a unified datacube."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Define common bucketing parameters\nlat_bucket_size = 0.5  # degrees\nlon_bucket_size = 0.5  # degrees\ntime_bucket_size = '1M'  # monthly\n\n# Process each dataset\nprocessed_datasets = {}\nfor scenario, processor in processors.items():\n    try:\n        processed_datasets[scenario] = processor.process_to_datacube(\n            lat_bucket_size=lat_bucket_size,\n            lon_bucket_size=lon_bucket_size,\n            time_bucket_size=time_bucket_size,\n            spatial_agg_method=AggregationMethod.MEAN,\n            temporal_agg_method=AggregationMethod.MEAN\n        )\n        print(f\"Processed {scenario} dataset with new dimensions: {processed_datasets[scenario].dims}\")\n    except Exception as e:\n        print(f\"Error processing {scenario} dataset: {e}\")\n        print(f\"Attempting simplified processing for {scenario}...\")\n        \n        try:\n            # Calculate stride for lat/lon to approximate the requested bucket sizes\n            lat_res, lon_res = processor.get_spatial_resolution()\n            lat_stride = max(1, int(lat_bucket_size / lat_res))\n            lon_stride = max(1, int(lon_bucket_size / lon_res))\n            time_stride = 1  # Monthly is typically the native resolution\n            \n            print(f\"Using strides: lat={lat_stride}, lon={lon_stride}, time={time_stride}\")\n            \n            # Create a coarser resolution dataset using slicing\n            simplified = processor.dataset.isel(\n                lat=slice(0, None, lat_stride),\n                lon=slice(0, None, lon_stride),\n                time=slice(0, None, time_stride)\n            )\n            processed_datasets[scenario] = simplified\n            print(f\"Simplified processing complete for {scenario}. New dimensions: {simplified.dims}\")\n        except Exception as nested_e:\n            print(f\"Failed to process {scenario} dataset even with simplified approach: {nested_e}\")\n            print(f\"Skipping {scenario} dataset\")\n\nif not processed_datasets:\n    print(\"No datasets were successfully processed. Cannot continue with datacube building.\")\nelse:\n    print(f\"Successfully processed {len(processed_datasets)} datasets: {', '.join(processed_datasets.keys())}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Unified Datacube\n",
    "\n",
    "Now, let's use the DatacubeBuilder to combine the processed datasets into a unified datacube."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check if we have processed datasets\nif not processed_datasets:\n    print(\"No processed datasets available. Cannot build unified datacube.\")\nelse:\n    # Initialize the datacube builder\n    builder = DatacubeBuilder()\n\n    # Add the processed datasets\n    for scenario, dataset in processed_datasets.items():\n        builder.add_dataset(scenario, dataset)\n\n    # Build the unified datacube\n    try:\n        unified_datacube = builder.build_datacube(\n            lat_resolution=lat_bucket_size,\n            lon_resolution=lon_bucket_size,\n            time_resolution=time_bucket_size,\n            interpolation_method=InterpolationMethod.LINEAR\n        )\n        \n        # Print information about the unified datacube\n        print(\"Unified Datacube Information:\")\n        print(unified_datacube)\n    except Exception as e:\n        print(f\"Error building unified datacube: {e}\")\n        print(\"Attempting simplified datacube creation...\")\n        \n        # Try a simpler approach if unified datacube building fails\n        try:\n            # Create a simplified datacube by concatenating datasets along a new dimension\n            datasets_list = list(processed_datasets.values())\n            scenarios_list = list(processed_datasets.keys())\n            \n            if len(datasets_list) > 1:\n                # For multiple datasets, concatenate along a new \"scenario\" dimension\n                unified_datacube = xr.concat(datasets_list, dim=\"scenario\")\n                unified_datacube = unified_datacube.assign_coords(scenario=scenarios_list)\n                print(\"Created simplified datacube by concatenating along 'scenario' dimension\")\n            else:\n                # For single dataset, just use as is\n                unified_datacube = datasets_list[0]\n                print(f\"Using single '{scenarios_list[0]}' dataset as the unified datacube\")\n            \n            print(\"Simplified Datacube Information:\")\n            print(unified_datacube)\n        except Exception as nested_e:\n            print(f\"Failed to create even a simplified datacube: {nested_e}\")\n            print(\"Cannot continue with datacube exploration without a unified datacube.\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Unified Datacube\n",
    "\n",
    "Let's explore the unified datacube to see how it combines the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check if unified_datacube is defined\nif 'unified_datacube' in locals():\n    # List all variables in the unified datacube\n    print(\"Variables in the unified datacube:\")\n    for var_name in unified_datacube.data_vars:\n        print(f\"  - {var_name}\")\nelse:\n    print(\"No unified datacube available for exploration.\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Variables Across Scenarios\n",
    "\n",
    "Let's compare the same variable across different scenarios in the unified datacube."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check if unified_datacube is defined\nif 'unified_datacube' not in locals():\n    print(\"No unified datacube available for time series analysis.\")\nelse:\n    # Get available variables\n    var_names = list(unified_datacube.data_vars)\n    \n    if not var_names:\n        print(\"No variables available in the unified datacube.\")\n    else:\n        # Check if we have both RCP4.5 and RCP8.5 specific variables\n        rcp45_vars = [v for v in var_names if 'rcp45' in v]\n        rcp85_vars = [v for v in var_names if 'rcp85' in v]\n        \n        if rcp45_vars and rcp85_vars:\n            # Find a common variable (without the scenario prefix)\n            common_var_base = None\n            for v45 in rcp45_vars:\n                for v85 in rcp85_vars:\n                    if v45.replace('rcp45_', '') == v85.replace('rcp85_', ''):\n                        common_var_base = v45.replace('rcp45_', '')\n                        break\n                if common_var_base:\n                    break\n            \n            if common_var_base:\n                # Get the full variable names\n                rcp45_var = f\"rcp45_{common_var_base}\"\n                rcp85_var = f\"rcp85_{common_var_base}\"\n                \n                # Select a specific location for time series analysis\n                lat_idx = len(unified_datacube.lat) // 2  # Middle latitude index\n                lon_idx = len(unified_datacube.lon) // 2  # Middle longitude index\n                \n                # Extract lat/lon values\n                lat_val = float(unified_datacube.lat[lat_idx])\n                lon_val = float(unified_datacube.lon[lon_idx])\n                \n                # Extract time series for this location\n                try:\n                    rcp45_series = unified_datacube[rcp45_var].isel(lat=lat_idx, lon=lon_idx)\n                    rcp85_series = unified_datacube[rcp85_var].isel(lat=lat_idx, lon=lon_idx)\n                    \n                    # Plot time series comparison\n                    plt.figure(figsize=(12, 6))\n                    rcp45_series.plot(label=\"RCP4.5\")\n                    rcp85_series.plot(label=\"RCP8.5\", linestyle=\"--\")\n                    plt.title(f\"{common_var_base} Time Series at Lat: {lat_val:.2f}, Lon: {lon_val:.2f}\")\n                    plt.xlabel('Time')\n                    plt.ylabel(f\"{common_var_base} (units)\")\n                    plt.grid(True)\n                    plt.legend()\n                    plt.tight_layout()\n                    plt.show()\n                except Exception as e:\n                    print(f\"Error extracting or plotting time series: {e}\")\n                    print(\"Attempting simplified time series extraction...\")\n                    \n                    try:\n                        # Try different indexing approach\n                        if 'scenario' in unified_datacube.dims:\n                            # For simplified concatenated datacube\n                            rcp45_idx = list(unified_datacube.scenario.values).index('rcp45')\n                            rcp85_idx = list(unified_datacube.scenario.values).index('rcp85')\n                            \n                            var_name = list(unified_datacube.data_vars)[0]\n                            rcp45_series = unified_datacube[var_name].isel(scenario=rcp45_idx, lat=lat_idx, lon=lon_idx)\n                            rcp85_series = unified_datacube[var_name].isel(scenario=rcp85_idx, lat=lat_idx, lon=lon_idx)\n                            \n                            # Plot time series comparison\n                            plt.figure(figsize=(12, 6))\n                            rcp45_series.plot(label=\"RCP4.5\")\n                            rcp85_series.plot(label=\"RCP8.5\", linestyle=\"--\")\n                            plt.title(f\"{var_name} Time Series at Lat: {lat_val:.2f}, Lon: {lon_val:.2f}\")\n                            plt.xlabel('Time')\n                            plt.ylabel(f\"{var_name} (units)\")\n                            plt.grid(True)\n                            plt.legend()\n                            plt.tight_layout()\n                            plt.show()\n                        else:\n                            raise ValueError(\"No scenario dimension in the datacube\")\n                    except Exception as nested_e:\n                        print(f\"Failed to extract time series with alternative method: {nested_e}\")\n            else:\n                print(\"No common variable found between RCP4.5 and RCP8.5 datasets.\")\n        else:\n            # Handle case with only one scenario\n            # Use the first available variable\n            var_name = var_names[0]\n            \n            # Check if 'lat' and 'lon' dimensions exist\n            if 'lat' in unified_datacube.dims and 'lon' in unified_datacube.dims:\n                lat_idx = len(unified_datacube.lat) // 2  # Middle latitude index\n                lon_idx = len(unified_datacube.lon) // 2  # Middle longitude index\n                \n                # Extract lat/lon values\n                lat_val = float(unified_datacube.lat[lat_idx])\n                lon_val = float(unified_datacube.lon[lon_idx])\n                \n                # Extract time series for this location\n                try:\n                    time_series = unified_datacube[var_name].isel(lat=lat_idx, lon=lon_idx)\n                    \n                    # Plot time series\n                    plt.figure(figsize=(12, 6))\n                    time_series.plot()\n                    plt.title(f\"{var_name} Time Series at Lat: {lat_val:.2f}, Lon: {lon_val:.2f}\")\n                    plt.xlabel('Time')\n                    plt.ylabel(f\"{var_name} (units)\")\n                    plt.grid(True)\n                    plt.tight_layout()\n                    plt.show()\n                except Exception as e:\n                    print(f\"Error plotting time series: {e}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Visualize Scenario Differences\n",
    "\n",
    "Let's calculate the difference between scenarios across the entire spatial domain over time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check if unified_datacube is defined\nif 'unified_datacube' not in locals():\n    print(\"No unified datacube available for calculating scenario differences.\")\nelse:\n    # Check if we have both RCP4.5 and RCP8.5 specific variables\n    rcp45_vars = [v for v in unified_datacube.data_vars if 'rcp45' in v]\n    rcp85_vars = [v for v in unified_datacube.data_vars if 'rcp85' in v]\n    \n    if not (rcp45_vars and rcp85_vars):\n        # Check if we have a scenario dimension instead\n        if 'scenario' in unified_datacube.dims and len(unified_datacube.scenario) >= 2:\n            # For simplified concatenated datacube\n            try:\n                rcp45_idx = list(unified_datacube.scenario.values).index('rcp45')\n                rcp85_idx = list(unified_datacube.scenario.values).index('rcp85')\n                \n                var_name = list(unified_datacube.data_vars)[0]\n                \n                # Calculate difference between scenarios\n                scenario_difference = unified_datacube[var_name].isel(scenario=rcp85_idx) - unified_datacube[var_name].isel(scenario=rcp45_idx)\n                \n                # Plot the spatial pattern of differences for the first time point\n                plt.figure(figsize=(12, 8))\n                scenario_difference.isel(time=0).plot(cmap='RdBu_r')\n                plt.title(f\"Difference in {var_name} (RCP8.5 - RCP4.5) at {unified_datacube.time.values[0]}\")\n                plt.tight_layout()\n                plt.show()\n                \n                # Create a diff dataset for consistency with later code\n                diff_dataset = xr.Dataset(\n                    data_vars={\n                        f\"{var_name}_difference\": scenario_difference\n                    },\n                    coords={\n                        'lat': unified_datacube.lat,\n                        'lon': unified_datacube.lon,\n                        'time': unified_datacube.time\n                    }\n                )\n            except Exception as e:\n                print(f\"Error calculating scenario differences: {e}\")\n                print(\"Cannot create difference visualization.\")\n        else:\n            print(\"Multiple scenarios (RCP4.5 and RCP8.5) are required to calculate differences.\")\n            print(\"Only found variables:\", list(unified_datacube.data_vars))\n    else:\n        # Find a common variable (without the scenario prefix)\n        common_var_base = None\n        for v45 in rcp45_vars:\n            for v85 in rcp85_vars:\n                if v45.replace('rcp45_', '') == v85.replace('rcp85_', ''):\n                    common_var_base = v45.replace('rcp45_', '')\n                    break\n            if common_var_base:\n                break\n        \n        if common_var_base:\n            # Get the full variable names\n            rcp45_var = f\"rcp45_{common_var_base}\"\n            rcp85_var = f\"rcp85_{common_var_base}\"\n            \n            try:\n                # Calculate difference between scenarios\n                scenario_difference = unified_datacube[rcp85_var] - unified_datacube[rcp45_var]\n                \n                # Create a new dataset with the difference\n                diff_dataset = xr.Dataset(\n                    data_vars={\n                        f\"{common_var_base}_difference\": scenario_difference\n                    },\n                    coords=unified_datacube.coords\n                )\n                \n                # Plot the spatial pattern of differences for the first time point\n                plt.figure(figsize=(12, 8))\n                diff_dataset[f\"{common_var_base}_difference\"].isel(time=0).plot(cmap='RdBu_r')\n                plt.title(f\"Difference in {common_var_base} (RCP8.5 - RCP4.5) at {unified_datacube.time.values[0]}\")\n                plt.tight_layout()\n                plt.show()\n            except Exception as e:\n                print(f\"Error calculating scenario differences: {e}\")\n                print(\"Cannot create difference visualization.\")\n        else:\n            print(\"No common variable found between RCP4.5 and RCP8.5 datasets.\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Temporal Trends\n",
    "\n",
    "Let's analyze how the difference between scenarios evolves over time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check if diff_dataset is defined\nif 'diff_dataset' not in locals():\n    print(\"No difference dataset available for temporal trend analysis.\")\nelse:\n    try:\n        # Find the difference variable\n        diff_var = [v for v in diff_dataset.data_vars if 'difference' in v]\n        if diff_var:\n            # Take the first difference variable found\n            diff_var = diff_var[0]\n            \n            # Calculate the spatial mean difference for each time point\n            mean_diff_over_time = diff_dataset[diff_var].mean(dim=['lat', 'lon'])\n            \n            # Plot the trend\n            plt.figure(figsize=(12, 6))\n            mean_diff_over_time.plot(marker='o')\n            plt.title(f\"Mean Difference in {diff_var.replace('_difference', '')} (RCP8.5 - RCP4.5) Over Time\")\n            plt.xlabel('Time')\n            plt.ylabel(f\"Mean Difference ({diff_var.replace('_difference', '')} units)\")\n            plt.grid(True)\n            plt.tight_layout()\n            plt.show()\n        else:\n            print(\"No difference variable found in the difference dataset.\")\n    except Exception as e:\n        print(f\"Error analyzing temporal trends: {e}\")\n        print(\"Cannot create temporal trend visualization.\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Multi-Variable Analysis\n",
    "\n",
    "If there are multiple variables in the datasets, let's analyze relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check if unified_datacube is defined\nif 'unified_datacube' not in locals():\n    print(\"No unified datacube available for multi-variable analysis.\")\nelse:\n    # Get all variables\n    all_vars = list(unified_datacube.data_vars)\n    \n    # Check if there are multiple variables\n    if len(all_vars) <= 1:\n        # For scenario dimension cube\n        if 'scenario' in unified_datacube.dims and len(unified_datacube.scenario) >= 2:\n            var_name = all_vars[0]\n            try:\n                # Compare same variable across scenarios\n                scenarios = list(unified_datacube.scenario.values)\n                if len(scenarios) >= 2:\n                    scenario1 = scenarios[0]\n                    scenario2 = scenarios[1]\n                    \n                    # Extract data for a specific time point\n                    time_idx = 0\n                    var1_data = unified_datacube[var_name].isel(scenario=0, time=time_idx).values.flatten()\n                    var2_data = unified_datacube[var_name].isel(scenario=1, time=time_idx).values.flatten()\n                    \n                    # Remove NaN values\n                    mask = ~(np.isnan(var1_data) | np.isnan(var2_data))\n                    var1_data = var1_data[mask]\n                    var2_data = var2_data[mask]\n                    \n                    if len(var1_data) > 0 and len(var2_data) > 0:\n                        # Create scatter plot\n                        plt.figure(figsize=(10, 8))\n                        plt.scatter(var1_data, var2_data, alpha=0.5)\n                        plt.title(f\"Relationship between {var_name} in {scenario1} vs {scenario2}\\nat {unified_datacube.time.values[time_idx]}\")\n                        plt.xlabel(f\"{var_name} in {scenario1}\")\n                        plt.ylabel(f\"{var_name} in {scenario2}\")\n                        plt.grid(True)\n                        plt.tight_layout()\n                        plt.show()\n                        \n                        # Calculate correlation\n                        corr = np.corrcoef(var1_data, var2_data)[0, 1]\n                        print(f\"Correlation between {var_name} in {scenario1} and {scenario2}: {corr:.4f}\")\n                    else:\n                        print(\"Not enough valid data points for analysis after removing NaN values.\")\n                else:\n                    print(\"Need at least two scenarios for comparison.\")\n            except Exception as e:\n                print(f\"Error in multi-variable analysis: {e}\")\n        else:\n            print(\"Not enough variables for multi-variable analysis. Only found:\", all_vars)\n    else:\n        # Select two variables for comparison\n        var1 = all_vars[0]\n        var2 = all_vars[1]\n        \n        try:\n            # Extract data for a specific time point\n            time_idx = 0\n            var1_data = unified_datacube[var1].isel(time=time_idx).values.flatten()\n            var2_data = unified_datacube[var2].isel(time=time_idx).values.flatten()\n            \n            # Remove NaN values\n            mask = ~(np.isnan(var1_data) | np.isnan(var2_data))\n            var1_data = var1_data[mask]\n            var2_data = var2_data[mask]\n            \n            if len(var1_data) > 0 and len(var2_data) > 0:\n                # Create scatter plot\n                plt.figure(figsize=(10, 8))\n                plt.scatter(var1_data, var2_data, alpha=0.5)\n                plt.title(f\"Relationship between {var1} and {var2} at {unified_datacube.time.values[time_idx]}\")\n                plt.xlabel(var1)\n                plt.ylabel(var2)\n                plt.grid(True)\n                plt.tight_layout()\n                plt.show()\n                \n                # Calculate correlation\n                corr = np.corrcoef(var1_data, var2_data)[0, 1]\n                print(f\"Correlation between {var1} and {var2}: {corr:.4f}\")\n            else:\n                print(\"Not enough valid data points for analysis after removing NaN values.\")\n        except Exception as e:\n            print(f\"Error in multi-variable analysis: {e}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Unified Datacube\n",
    "\n",
    "Finally, let's save the unified datacube for future use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check if we have the necessary datasets to save\nif 'unified_datacube' not in locals():\n    print(\"No unified datacube available to save.\")\nelse:\n    try:\n        # Create output directory if it doesn't exist\n        os.makedirs(\"./data/processed\", exist_ok=True)\n        \n        # Save the unified datacube\n        output_path = \"./data/processed/unified_cmip_datacube.nc\"\n        \n        if hasattr(builder, 'save_datacube'):\n            # If we used the DatacubeBuilder\n            builder.save_datacube(output_path)\n            print(f\"Saved unified datacube to {output_path}\")\n        else:\n            # Direct save\n            unified_datacube.to_netcdf(output_path)\n            print(f\"Saved unified datacube to {output_path}\")\n            \n        # Also save the difference dataset if available\n        if 'diff_dataset' in locals():\n            try:\n                diff_output_path = \"./data/processed/scenario_difference_datacube.nc\"\n                diff_dataset.to_netcdf(diff_output_path)\n                print(f\"Saved scenario difference dataset to {diff_output_path}\")\n            except Exception as e:\n                print(f\"Error saving difference dataset: {e}\")\n    except Exception as e:\n        print(f\"Error saving datacube: {e}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Load and process multiple CMIP climate datasets\n",
    "2. Compare the datasets and visualize their differences\n",
    "3. Build a unified datacube that combines multiple datasets\n",
    "4. Explore the unified datacube through various visualizations and analyses\n",
    "5. Calculate and visualize differences between climate scenarios\n",
    "6. Save the unified datacube for future use\n",
    "\n",
    "This approach can be extended to include additional datasets and variables, creating a comprehensive datacube for climate analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}