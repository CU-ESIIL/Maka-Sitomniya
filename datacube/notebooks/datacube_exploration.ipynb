{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datacube Exploration\n",
    "\n",
    "This notebook demonstrates how to combine multiple CMIP datasets into a unified datacube and explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the scripts directory to the path so we can import our modules\n",
    "sys.path.append('../scripts')\n",
    "from cmip_processor import CMIPProcessor, AggregationMethod\n",
    "from datacube_builder import DatacubeBuilder, InterpolationMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CMIP Datasets\n",
    "\n",
    "First, let's load the available CMIP datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Paths to the CMIP data files\ndata_files = {\n    \"rcp45\": \"./data/macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_monthly.nc\",\n    \"rcp85\": \"./data/macav2metdata_huss_CCSM4_r6i1p1_rcp85_2021_2025_CONUS_monthly.nc\"\n}\n\n# Check for missing data files\nmissing_files = []\nfor scenario, file_path in data_files.items():\n    if not os.path.exists(file_path):\n        missing_files.append(file_path)\n\nif missing_files:\n    raise FileNotFoundError(f\"Required CMIP data files not found: {', '.join(missing_files)}. Please ensure you're running this notebook from the datacube directory.\")\n\n# Helper function to load datasets with non-standard calendars\ndef load_dataset_with_calendar_handling(file_path):\n    try:\n        # Try the standard CMIPProcessor\n        processor = CMIPProcessor(file_path)\n        return processor\n    except ValueError as e:\n        if \"unable to decode time units\" in str(e) or \"calendar\" in str(e):\n            print(f\"Warning: Non-standard calendar detected in {file_path}\")\n            print(\"Attempting to fix by manually loading the dataset...\")\n            \n            # Override CMIPProcessor._load_dataset method to handle non-standard calendars\n            import xarray as xr\n            \n            try:\n                # Try to import cftime\n                import cftime\n                print(\"Using cftime for non-standard calendar.\")\n                dataset = xr.open_dataset(file_path, use_cftime=True)\n            except ImportError:\n                print(\"cftime not available. Loading without decoding times.\")\n                dataset = xr.open_dataset(file_path, decode_times=False)\n            \n            # Create a processor and manually set the dataset\n            processor = CMIPProcessor(file_path)\n            processor.dataset = dataset\n            return processor\n        else:\n            # Re-raise if it's a different ValueError\n            raise\n\n# Initialize processors for each dataset\nprocessors = {}\nfor scenario, file_path in data_files.items():\n    processors[scenario] = load_dataset_with_calendar_handling(file_path)\n    print(f\"Loaded {scenario} dataset with dimensions: {processors[scenario].dataset.dims}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Datasets\n",
    "\n",
    "Let's compare basic statistics between the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get the first variable from the datasets\n",
    "var_name = list(processors[\"rcp45\"].dataset.data_vars)[0]\n",
    "\n",
    "# Compare statistics\n",
    "stats = []\n",
    "for scenario, processor in processors.items():\n",
    "    var = processor.dataset[var_name]\n",
    "    stats.append({\n",
    "        \"Scenario\": scenario,\n",
    "        \"Min\": float(var.min()),\n",
    "        \"Max\": float(var.max()),\n",
    "        \"Mean\": float(var.mean()),\n",
    "        \"Std\": float(var.std())\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Differences Between Scenarios\n",
    "\n",
    "Let's create maps to visualize the differences between the RCP4.5 and RCP8.5 scenarios."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Calculate difference between scenarios for the first time point\nrcp45_data = processors[\"rcp45\"].dataset[var_name].isel(time=0)\nrcp85_data = processors[\"rcp85\"].dataset[var_name].isel(time=0)\n\n# Check coordinate systems\nprint(\"RCP45 coordinates:\", rcp45_data.lat.shape, rcp45_data.lon.shape)\nprint(\"RCP85 coordinates:\", rcp85_data.lat.shape, rcp85_data.lon.shape)\n\n# Ensure they have the same coordinates\nif not (rcp45_data.lat.equals(rcp85_data.lat) and rcp45_data.lon.equals(rcp85_data.lon)):\n    print(\"Warning: Datasets have different coordinates.\")\n    \n    # If dimensions are identical but values differ\n    if rcp45_data.lat.shape == rcp85_data.lat.shape and rcp45_data.lon.shape == rcp85_data.lon.shape:\n        print(\"Dimensions match but values differ. Using as-is.\")\n        diff_data = rcp85_data - rcp45_data\n    else:\n        # Check which has the coarser resolution\n        if len(rcp45_data.lat) <= len(rcp85_data.lat) and len(rcp45_data.lon) <= len(rcp85_data.lon):\n            # RCP45 is coarser, so downsample RCP85\n            print(\"RCP45 has coarser resolution. Downsampling RCP85...\")\n            rcp85_data = rcp85_data.sel(\n                lat=rcp45_data.lat.values,\n                lon=rcp45_data.lon.values,\n                method=\"nearest\"\n            )\n        else:\n            # RCP85 is coarser, so downsample RCP45\n            print(\"RCP85 has coarser resolution. Downsampling RCP45...\")\n            rcp45_data = rcp45_data.sel(\n                lat=rcp85_data.lat.values,\n                lon=rcp85_data.lon.values,\n                method=\"nearest\"\n            )\n        \n        # Calculate difference\n        diff_data = rcp85_data - rcp45_data\nelse:\n    # Calculate difference if coordinates are already the same\n    diff_data = rcp85_data - rcp45_data\n\n# Create a 1x3 subplot for comparison\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Plot RCP4.5\nrcp45_plot = rcp45_data.plot(ax=axes[0], cmap='viridis')\naxes[0].set_title(f\"RCP4.5 {var_name} (First Month)\")\n\n# Plot RCP8.5\nrcp85_plot = rcp85_data.plot(ax=axes[1], cmap='viridis')\naxes[1].set_title(f\"RCP8.5 {var_name} (First Month)\")\n\n# Plot Difference\ndiff_plot = diff_data.plot(ax=axes[2], cmap='RdBu_r')\naxes[2].set_title(f\"Difference (RCP8.5 - RCP4.5)\")\n\nplt.tight_layout()\nplt.show()",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Datasets for Datacube Integration\n",
    "\n",
    "Now let's process each dataset to prepare for integration into a unified datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define common bucketing parameters\n",
    "lat_bucket_size = 0.5  # degrees\n",
    "lon_bucket_size = 0.5  # degrees\n",
    "time_bucket_size = '1M'  # monthly\n",
    "\n",
    "# Process each dataset\n",
    "processed_datasets = {}\n",
    "for scenario, processor in processors.items():\n",
    "    processed_datasets[scenario] = processor.process_to_datacube(\n",
    "        lat_bucket_size=lat_bucket_size,\n",
    "        lon_bucket_size=lon_bucket_size,\n",
    "        time_bucket_size=time_bucket_size,\n",
    "        spatial_agg_method=AggregationMethod.MEAN,\n",
    "        temporal_agg_method=AggregationMethod.MEAN\n",
    "    )\n",
    "    print(f\"Processed {scenario} dataset with new dimensions: {processed_datasets[scenario].dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Unified Datacube\n",
    "\n",
    "Now, let's use the DatacubeBuilder to combine the processed datasets into a unified datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the datacube builder\n",
    "builder = DatacubeBuilder()\n",
    "\n",
    "# Add the processed datasets\n",
    "for scenario, dataset in processed_datasets.items():\n",
    "    builder.add_dataset(scenario, dataset)\n",
    "\n",
    "# Build the unified datacube\n",
    "unified_datacube = builder.build_datacube(\n",
    "    lat_resolution=lat_bucket_size,\n",
    "    lon_resolution=lon_bucket_size,\n",
    "    time_resolution=time_bucket_size,\n",
    "    interpolation_method=InterpolationMethod.LINEAR\n",
    ")\n",
    "\n",
    "# Print information about the unified datacube\n",
    "print(\"Unified Datacube Information:\")\n",
    "print(unified_datacube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Unified Datacube\n",
    "\n",
    "Let's explore the unified datacube to see how it combines the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List all variables in the unified datacube\n",
    "print(\"Variables in the unified datacube:\")\n",
    "for var_name in unified_datacube.data_vars:\n",
    "    print(f\"  - {var_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Variables Across Scenarios\n",
    "\n",
    "Let's compare the same variable across different scenarios in the unified datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify the common variable name across scenarios\n",
    "common_var = list(processors[\"rcp45\"].dataset.data_vars)[0]\n",
    "\n",
    "# Get the variable names in the unified datacube for each scenario\n",
    "rcp45_var = f\"rcp45_{common_var}\"\n",
    "rcp85_var = f\"rcp85_{common_var}\"\n",
    "\n",
    "# Select a specific location for time series analysis\n",
    "lat_idx = len(unified_datacube.lat) // 2  # Middle latitude index\n",
    "lon_idx = len(unified_datacube.lon) // 2  # Middle longitude index\n",
    "\n",
    "# Extract lat/lon values\n",
    "lat_val = float(unified_datacube.lat[lat_idx])\n",
    "lon_val = float(unified_datacube.lon[lon_idx])\n",
    "\n",
    "# Extract time series for this location\n",
    "rcp45_series = unified_datacube[rcp45_var].isel(lat=lat_idx, lon=lon_idx)\n",
    "rcp85_series = unified_datacube[rcp85_var].isel(lat=lat_idx, lon=lon_idx)\n",
    "\n",
    "# Plot time series comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "rcp45_series.plot(label=\"RCP4.5\")\n",
    "rcp85_series.plot(label=\"RCP8.5\", linestyle=\"--\")\n",
    "plt.title(f\"{common_var} Time Series at Lat: {lat_val:.2f}, Lon: {lon_val:.2f}\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(f\"{common_var} (units)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Visualize Scenario Differences\n",
    "\n",
    "Let's calculate the difference between scenarios across the entire spatial domain over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate difference between scenarios\n",
    "scenario_difference = unified_datacube[rcp85_var] - unified_datacube[rcp45_var]\n",
    "\n",
    "# Create a new dataset with the difference\n",
    "diff_dataset = xr.Dataset(\n",
    "    data_vars={\n",
    "        f\"{common_var}_difference\": scenario_difference\n",
    "    },\n",
    "    coords=unified_datacube.coords\n",
    ")\n",
    "\n",
    "# Plot the spatial pattern of differences for the first time point\n",
    "plt.figure(figsize=(12, 8))\n",
    "diff_dataset[f\"{common_var}_difference\"].isel(time=0).plot(cmap='RdBu_r')\n",
    "plt.title(f\"Difference in {common_var} (RCP8.5 - RCP4.5) at {unified_datacube.time.values[0]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Temporal Trends\n",
    "\n",
    "Let's analyze how the difference between scenarios evolves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate the spatial mean difference for each time point\n",
    "mean_diff_over_time = diff_dataset[f\"{common_var}_difference\"].mean(dim=['lat', 'lon'])\n",
    "\n",
    "# Plot the trend\n",
    "plt.figure(figsize=(12, 6))\n",
    "mean_diff_over_time.plot(marker='o')\n",
    "plt.title(f\"Mean Difference in {common_var} (RCP8.5 - RCP4.5) Over Time\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(f\"Mean Difference ({common_var} units)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Multi-Variable Analysis\n",
    "\n",
    "If there are multiple variables in the datasets, let's analyze relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if there are multiple variables\n",
    "all_vars = list(unified_datacube.data_vars)\n",
    "if len(all_vars) > 2:  # Need at least two variables to compare\n",
    "    # Select two variables for comparison\n",
    "    var1 = all_vars[0]\n",
    "    var2 = all_vars[1]\n",
    "    \n",
    "    # Extract data for a specific time point\n",
    "    time_idx = 0\n",
    "    var1_data = unified_datacube[var1].isel(time=time_idx).values.flatten()\n",
    "    var2_data = unified_datacube[var2].isel(time=time_idx).values.flatten()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    mask = ~(np.isnan(var1_data) | np.isnan(var2_data))\n",
    "    var1_data = var1_data[mask]\n",
    "    var2_data = var2_data[mask]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(var1_data, var2_data, alpha=0.5)\n",
    "    plt.title(f\"Relationship between {var1} and {var2} at {unified_datacube.time.values[time_idx]}\")\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = np.corrcoef(var1_data, var2_data)[0, 1]\n",
    "    print(f\"Correlation between {var1} and {var2}: {corr:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough variables for multi-variable analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Unified Datacube\n",
    "\n",
    "Finally, let's save the unified datacube for future use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Save the unified datacube\noutput_path = \"./data/processed/unified_cmip_datacube.nc\"\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nbuilder.save_datacube(output_path)\nprint(f\"Saved unified datacube to {output_path}\")\n\n# Also save the difference dataset\ndiff_output_path = \"./data/processed/scenario_difference_datacube.nc\"\ndiff_dataset.to_netcdf(diff_output_path)\nprint(f\"Saved scenario difference dataset to {diff_output_path}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Load and process multiple CMIP climate datasets\n",
    "2. Compare the datasets and visualize their differences\n",
    "3. Build a unified datacube that combines multiple datasets\n",
    "4. Explore the unified datacube through various visualizations and analyses\n",
    "5. Calculate and visualize differences between climate scenarios\n",
    "6. Save the unified datacube for future use\n",
    "\n",
    "This approach can be extended to include additional datasets and variables, creating a comprehensive datacube for climate analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}