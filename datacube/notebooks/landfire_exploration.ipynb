{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LANDFIRE Existing Vegetation Type (EVT) Data Exploration\n",
        "\n",
        "TODO: FIX, CURRENTLY BROKEN DOWNLOAD. This notebook explores the LANDFIRE Existing Vegetation Type (EVT) data and demonstrates how to integrate it into our datacube framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Add the scripts directory to the path so we can import our modules\n",
        "sys.path.append('../scripts')\n",
        "from landfire_processor import LandfireProcessor, AggregationMethod\n",
        "from datacube_builder import DatacubeBuilder, InterpolationMethod"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download and Process LANDFIRE EVT Data\n",
        "\n",
        "First, let's download and process LANDFIRE Existing Vegetation Type (EVT) data for a specific area. We'll use a small region in South Dakota as an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Try to import configuration if available\ntry:\n    # Add parent directory to path to import config\n    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n    import config\n    \n    # Use config values if available\n    if hasattr(config, 'BLACK_HILLS_BBOX_STRING'):\n        bbox = config.BLACK_HILLS_BBOX_STRING\n        print(f\"Using bounding box from config: {bbox}\")\n    else:\n        # Define our area of interest (South Dakota region)\n        bbox = \"-103.50 43.00 -102.00 44.00\"\n        print(f\"Using default bounding box: {bbox}\")\n    \n    # Use LANDFIRE config if available\n    if hasattr(config, 'LANDFIRE_CONFIG'):\n        data_dir = config.LANDFIRE_CONFIG.get(\"data_dir\", \"./data/landfire\")\n    else:\n        data_dir = \"./data/landfire\"\nexcept ImportError:\n    # Define our area of interest (South Dakota region)\n    bbox = \"-103.50 43.00 -102.00 44.00\"\n    data_dir = \"./data/landfire\"\n    print(\"Configuration module not found. Using default values.\")\n\n# Initialize the LANDFIRE processor\nprocessor = LandfireProcessor(bbox=bbox, data_dir=data_dir)\n\n# Check if the data directory exists\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir, exist_ok=True)\n    print(f\"Created LANDFIRE data directory: {data_dir}\")\n\n# Check if we have downloaded data already\nexpected_zip = os.path.join(data_dir, \"landfire_220EVT.zip\")\nif os.path.exists(expected_zip):\n    print(f\"Using existing downloaded data: {expected_zip}\")\n    processor.raw_data_path = expected_zip\nelse:\n    # Try to download the latest EVT data (2020 version)\n    try:\n        try:\n            # First check if the landfire package is available\n            import landfire\n            data_path = processor.download_data(\"220EVT\")\n            print(f\"Downloaded data to: {data_path}\")\n        except ImportError:\n            raise RuntimeError(\n                \"The landfire package is not installed. Cannot download LANDFIRE data.\\n\"\n                \"Please install it with: pip install landfire\\n\"\n                \"Or manually download LANDFIRE EVT data and place it at: \" + expected_zip\n            )\n    except Exception as e:\n        print(f\"Error downloading data: {e}\")\n        \n        # Check if we have any LANDFIRE data files that we could use instead\n        all_zips = glob.glob(os.path.join(data_dir, \"*.zip\"))\n        if all_zips:\n            alternative = all_zips[0]\n            print(f\"Using alternative LANDFIRE data file: {alternative}\")\n            processor.raw_data_path = alternative\n        else:\n            raise RuntimeError(\"No LANDFIRE data available. This notebook requires LANDFIRE EVT data to run.\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract and Load the Data\n",
        "\n",
        "Now, let's extract the data from the ZIP file and load it into an xarray Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Check if we already have the extracted data or dataset prepared\nif hasattr(processor, 'processed_dataset') and processor.processed_dataset is not None:\n    # If we've already processed\n    evt_dataset = processor.processed_dataset\n    print(\"Using already prepared dataset\")\nelif hasattr(processor, 'raw_dataset') and processor.raw_dataset is not None:\n    # If raw dataset is already loaded\n    evt_dataset = processor.raw_dataset\n    print(\"Using already loaded raw dataset\")\nelse:\n    try:\n        # Extract the data if we have a raw data path\n        if processor.raw_data_path:\n            try:\n                extracted_dir = processor.extract_data()\n                print(f\"Extracted data to: {extracted_dir}\")\n            except Exception as e:\n                print(f\"Error extracting data: {e}\")\n                print(\"Checking if data is already extracted...\")\n                \n                # Check if there's already extracted data\n                if hasattr(processor, 'tiff_path') and os.path.exists(processor.tiff_path):\n                    print(f\"Using already extracted data at: {processor.tiff_path}\")\n                else:\n                    # Look for any tiff files in the data directory\n                    tiff_files = glob.glob(os.path.join(data_dir, \"**/*.tif\"), recursive=True)\n                    if tiff_files:\n                        processor.tiff_path = tiff_files[0]\n                        print(f\"Found TIFF file: {processor.tiff_path}\")\n                    else:\n                        raise RuntimeError(\"No TIFF files found. Cannot proceed without extracted data.\")\n        \n        # Create an xarray Dataset from the GeoTIFF\n        try:\n            evt_dataset = processor.create_dataset()\n            print(f\"Dataset created with dimensions: {evt_dataset.dims}\")\n            print(f\"Variables: {list(evt_dataset.data_vars)}\")\n        except Exception as e:\n            print(f\"Error creating dataset: {e}\")\n            \n            # Try an alternative approach if the standard one fails\n            if not hasattr(processor, 'tiff_path') or not os.path.exists(processor.tiff_path):\n                raise RuntimeError(\"Failed to create dataset from LANDFIRE data and no TIFF file available.\")\n            \n            print(\"Attempting to create dataset with alternative approach...\")\n            try:\n                import rasterio\n                import xarray as xr\n                import numpy as np\n                \n                # Open the TIFF file with rasterio\n                with rasterio.open(processor.tiff_path) as src:\n                    # Read the data\n                    data = src.read(1)  # Read the first band\n                    \n                    # Create basic DataArray\n                    da = xr.DataArray(\n                        data=data,\n                        dims=[\"y\", \"x\"],\n                        coords={\n                            \"y\": np.arange(data.shape[0]),\n                            \"x\": np.arange(data.shape[1])\n                        },\n                        name=\"evt\"\n                    )\n                    \n                    # Create Dataset\n                    evt_dataset = xr.Dataset({\"evt\": da})\n                    print(\"Created simplified dataset from TIFF file\")\n            except Exception as nested_e:\n                print(f\"Alternative dataset creation also failed: {nested_e}\")\n                raise RuntimeError(\"Failed to create dataset from LANDFIRE data.\")\n    except Exception as e:\n        print(f\"Error processing LANDFIRE data: {e}\")\n        raise RuntimeError(\"Unable to process LANDFIRE data. This notebook requires valid LANDFIRE data to run.\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the Raw EVT Data\n",
        "\n",
        "Let's create a visualization of the raw EVT data before processing. EVT values are categorical, representing different vegetation types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create a simple colormap for EVT values\n",
        "# In a real application, you would use the actual EVT classification colors\n",
        "np.random.seed(42)  # For reproducible colors\n",
        "n_colors = 50  # Adjust based on number of unique values\n",
        "colors = np.random.rand(n_colors, 3)\n",
        "cmap = ListedColormap(colors)\n",
        "\n",
        "# Plot the EVT data\n",
        "plt.figure(figsize=(12, 8))\n",
        "evt_dataset.evt.plot(cmap=cmap)\n",
        "plt.title(\"LANDFIRE Existing Vegetation Type (EVT)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print some basic statistics\n",
        "unique_values = np.unique(evt_dataset.evt.values)\n",
        "print(f\"Number of unique EVT values: {len(unique_values)}\")\n",
        "print(f\"Example EVT values: {unique_values[:10]}...\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reproject to Latitude/Longitude Coordinates\n",
        "\n",
        "Next, let's reproject the dataset to standard latitude/longitude coordinates for easier integration with other datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Reproject the dataset to lat/lon\nreprojected = processor.reproject_to_latlon()\nprint(f\"Reprojected dataset coordinates: {list(reprojected.coords)}\")\nprint(f\"Latitude range: {float(reprojected.lat.min())} to {float(reprojected.lat.max())}\")\nprint(f\"Longitude range: {float(reprojected.lon.min())} to {float(reprojected.lon.max())}\")\n\n# Plot the reprojected data\nplt.figure(figsize=(12, 8))\nreprojected.evt.plot(cmap=cmap)\nplt.title(\"LANDFIRE EVT (Reprojected to Lat/Lon)\")\nplt.tight_layout()\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Spatial Bucketing\n",
        "\n",
        "Now, let's aggregate the data into spatial buckets to match our datacube framework. For categorical data like EVT, we'll use the MODE aggregation method (most common value in each bucket)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Define the bucket sizes\n",
        "lat_bucket_size = 0.05  # about 5 km\n",
        "lon_bucket_size = 0.05  # about 5 km\n",
        "\n",
        "# Bucket the data using MODE aggregation (most common value)\n",
        "bucketed = processor.bucket_spatial(\n",
        "    lat_bucket_size=lat_bucket_size,\n",
        "    lon_bucket_size=lon_bucket_size,\n",
        "    agg_method=AggregationMethod.MODE\n",
        ")\n",
        "\n",
        "print(f\"Bucketed dataset dimensions: {bucketed.dims}\")\n",
        "print(f\"Original dimensions: {processor.processed_dataset.dims}\")\n",
        "print(f\"Reduction factor: {processor.processed_dataset.sizes['lat'] * processor.processed_dataset.sizes['lon'] / (len(bucketed.lat_bins) * len(bucketed.lon_bins)):.2f}x\")\n",
        "\n",
        "# Plot the bucketed data\n",
        "plt.figure(figsize=(12, 8))\n",
        "bucketed.evt.plot(cmap=cmap)\n",
        "plt.title(f\"LANDFIRE EVT (Bucketed {lat_bucket_size}\u00b0 x {lon_bucket_size}\u00b0)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integrate with Datacube Framework\n",
        "\n",
        "Now, let's demonstrate how to integrate the LANDFIRE EVT data with our datacube framework by combining it with another dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# For demonstration, we'll use actual climate data if available\n# Check if we have CMIP data files available\ncmip_file = \"./data/macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_monthly.nc\"\n\n# Try alternative files if the main one doesn't exist\nif not os.path.exists(cmip_file):\n    alt_files = glob.glob(\"./data/macav2metdata_*_*_*_*_*_*.nc\")\n    if alt_files:\n        cmip_file = alt_files[0]\n        print(f\"Using alternative CMIP file: {cmip_file}\")\n\nif os.path.exists(cmip_file):\n    # Use the actual CMIP data\n    try:\n        temp_dataset = xr.open_dataset(cmip_file)\n        var_name = list(temp_dataset.data_vars)[0]\n        \n        # Check if the dataset has the necessary coordinates\n        if all(dim in temp_dataset.dims for dim in ['lat', 'lon', 'time']):\n            print(f\"Using actual CMIP data from: {cmip_file}\")\n            \n            # Subset the data to the same region as our LANDFIRE data\n            if 'lat' in reprojected.dims and 'lon' in reprojected.dims:\n                lat_min = float(reprojected.lat.min())\n                lat_max = float(reprojected.lat.max())\n                lon_min = float(reprojected.lon.min())\n                lon_max = float(reprojected.lon.max())\n                \n                # Find closest matching indices in the CMIP dataset\n                lat_indices = np.where((temp_dataset.lat >= lat_min) & (temp_dataset.lat <= lat_max))[0]\n                lon_indices = np.where((temp_dataset.lon >= lon_min) & (temp_dataset.lon <= lon_max))[0]\n                \n                if len(lat_indices) > 0 and len(lon_indices) > 0:\n                    # Subset to the region\n                    temp_dataset = temp_dataset.isel(\n                        lat=slice(lat_indices[0], lat_indices[-1]+1),\n                        lon=slice(lon_indices[0], lon_indices[-1]+1)\n                    )\n                    \n                    # Plot the temperature dataset for the first time point\n                    plt.figure(figsize=(10, 6))\n                    temp_dataset[var_name].isel(time=0).plot(cmap='viridis')\n                    plt.title(f\"CMIP Data: {var_name} (First Time Point)\")\n                    plt.tight_layout()\n                    plt.show()\n                else:\n                    print(\"No overlapping region found between LANDFIRE and CMIP data.\")\n                    raise ValueError(\"Region mismatch\")\n            else:\n                # Just plot the first time step\n                plt.figure(figsize=(10, 6))\n                temp_dataset[var_name].isel(time=0).plot(cmap='viridis')\n                plt.title(f\"CMIP Data: {var_name} (First Time Point)\")\n                plt.tight_layout()\n                plt.show()\n        else:\n            print(f\"CMIP dataset does not have required dimensions (lat, lon, time)\")\n            raise ValueError(\"Missing dimensions\")\n            \n    except Exception as e:\n        print(f\"Error loading or processing CMIP data: {e}\")\n        raise RuntimeError(\"Unable to use CMIP data for demonstration. This notebook requires valid CMIP data files.\")\nelse:\n    # No CMIP data available\n    raise FileNotFoundError(\n        \"No CMIP data files found for combined analysis. Please provide at least one CMIP data file.\"\n    )",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combine Datasets with DatacubeBuilder\n",
        "\n",
        "Now, let's use our DatacubeBuilder to combine the EVT and temperature datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Initialize the datacube builder\n",
        "builder = DatacubeBuilder()\n",
        "\n",
        "# Add our datasets\n",
        "builder.add_dataset(\"evt\", bucketed)\n",
        "builder.add_dataset(\"temperature\", temp_dataset)\n",
        "\n",
        "# Build a unified datacube\n",
        "unified_cube = builder.build_datacube(\n",
        "    lat_resolution=0.1,\n",
        "    lon_resolution=0.1,\n",
        "    time_resolution='1MS',  # Monthly\n",
        "    interpolation_method=InterpolationMethod.NEAREST  # Nearest neighbor is best for categorical data\n",
        ")\n",
        "\n",
        "print(f\"Unified datacube dimensions: {unified_cube.dims}\")\n",
        "print(f\"Variables: {list(unified_cube.data_vars)}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Relationships between Vegetation Type and Temperature\n",
        "\n",
        "Now that we have vegetation and temperature data in a unified datacube, we can start to analyze relationships between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Plot both datasets for visual comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "# Plot EVT\n",
        "unified_cube.evt_evt.isel(time=0).plot(ax=axes[0], cmap=cmap)\n",
        "axes[0].set_title(\"Existing Vegetation Type\")\n",
        "\n",
        "# Plot Temperature for same time\n",
        "unified_cube.temperature_temperature.isel(time=0).plot(ax=axes[1], cmap='viridis')\n",
        "axes[1].set_title(f\"Temperature ({unified_cube.time.values[0]})\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Statistics by Vegetation Type\n",
        "\n",
        "Let's compute temperature statistics for each vegetation type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# First, let's get a flattened view of the data for the first time step\n",
        "time_idx = 0\n",
        "evt_flat = unified_cube.evt_evt.isel(time=time_idx).values.flatten()\n",
        "temp_flat = unified_cube.temperature_temperature.isel(time=time_idx).values.flatten()\n",
        "\n",
        "# Remove NaN values\n",
        "mask = ~(np.isnan(evt_flat) | np.isnan(temp_flat))\n",
        "evt_flat = evt_flat[mask]\n",
        "temp_flat = temp_flat[mask]\n",
        "\n",
        "# Calculate statistics by vegetation type\n",
        "evt_types = np.unique(evt_flat)\n",
        "stats = []\n",
        "\n",
        "for evt_val in evt_types:\n",
        "    # Find all temperature values for this vegetation type\n",
        "    temps = temp_flat[evt_flat == evt_val]\n",
        "    \n",
        "    if len(temps) > 0:\n",
        "        stats.append({\n",
        "            \"EVT\": int(evt_val),\n",
        "            \"Count\": len(temps),\n",
        "            \"Min Temp\": temps.min(),\n",
        "            \"Max Temp\": temps.max(),\n",
        "            \"Mean Temp\": temps.mean(),\n",
        "            \"Std Temp\": temps.std()\n",
        "        })\n",
        "\n",
        "# Create a dataframe\n",
        "stats_df = pd.DataFrame(stats)\n",
        "stats_df = stats_df.sort_values(\"Mean Temp\", ascending=False)\n",
        "\n",
        "# Display the top 10 vegetation types by mean temperature\n",
        "stats_df.head(10)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Temperature Distribution by Vegetation Type\n",
        "\n",
        "Let's create a visualization of temperature distributions for different vegetation types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Select the top 5 most common vegetation types\n",
        "top_types = stats_df.sort_values(\"Count\", ascending=False).head(5)[\"EVT\"].values\n",
        "\n",
        "# Create a box plot of temperatures for each vegetation type\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "box_data = []\n",
        "labels = []\n",
        "\n",
        "for evt_val in top_types:\n",
        "    temps = temp_flat[evt_flat == evt_val]\n",
        "    box_data.append(temps)\n",
        "    labels.append(f\"EVT {evt_val}\")\n",
        "\n",
        "plt.boxplot(box_data, labels=labels)\n",
        "plt.title(\"Temperature Distribution by Vegetation Type\")\n",
        "plt.xlabel(\"Vegetation Type\")\n",
        "plt.ylabel(\"Temperature (\u00b0C)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time Series Analysis for a Specific Location\n",
        "\n",
        "Let's extract and analyze the temperature time series for a specific location and vegetation type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Find coordinates for a location with a specific vegetation type\n",
        "# First, get a specific vegetation type (use the most common one)\n",
        "common_evt = stats_df.sort_values(\"Count\", ascending=False).iloc[0][\"EVT\"]\n",
        "\n",
        "# Find the first occurrence of this type\n",
        "evt_data = unified_cube.evt_evt.isel(time=0)\n",
        "coords = np.where(evt_data.values == common_evt)\n",
        "\n",
        "if len(coords[0]) > 0:\n",
        "    # Get the lat/lon indices\n",
        "    lat_idx = coords[0][0]\n",
        "    lon_idx = coords[1][0]\n",
        "    \n",
        "    # Get actual coordinates\n",
        "    lat_val = float(unified_cube.lat[lat_idx])\n",
        "    lon_val = float(unified_cube.lon[lon_idx])\n",
        "    \n",
        "    print(f\"Selected location: Lat {lat_val:.4f}, Lon {lon_val:.4f}\")\n",
        "    print(f\"Vegetation type: {int(evt_data.values[lat_idx, lon_idx])}\")\n",
        "    \n",
        "    # Extract temperature time series\n",
        "    temp_series = unified_cube.temperature_temperature.sel(lat=lat_val, lon=lon_val, method=\"nearest\")\n",
        "    \n",
        "    # Plot the time series\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    temp_series.plot(marker='o')\n",
        "    plt.title(f\"Temperature Time Series for Vegetation Type {int(common_evt)}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Temperature (\u00b0C)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"No locations found with vegetation type {common_evt}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the Combined Datacube\n",
        "\n",
        "Finally, let's save our combined datacube for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Save the datacube to a NetCDF file\noutput_path = \"./data/processed/evt_temperature_datacube.nc\"\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nbuilder.save_datacube(output_path)\nprint(f\"Saved combined datacube to {output_path}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've demonstrated how to:\n",
        "\n",
        "1. Download and process LANDFIRE Existing Vegetation Type (EVT) data\n",
        "2. Convert the data to an xarray Dataset format\n",
        "3. Reproject the data to latitude/longitude coordinates\n",
        "4. Apply spatial bucketing using mode (most common value) aggregation for categorical data\n",
        "5. Combine the EVT data with temperature data in a unified datacube\n",
        "6. Analyze relationships between vegetation types and temperature\n",
        "7. Save the combined datacube for future use\n",
        "\n",
        "This approach can be extended to include additional datasets and to analyze more complex relationships between vegetation types and climate variables."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
