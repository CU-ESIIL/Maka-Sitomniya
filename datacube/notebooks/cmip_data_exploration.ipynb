{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMIP Data Exploration\n",
    "\n",
    "This notebook explores the CMIP climate data files and demonstrates how to work with them using our processing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the scripts directory to the path so we can import our modules\n",
    "sys.path.append('../scripts')\n",
    "from cmip_processor import CMIPProcessor, AggregationMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Examine the CMIP Data\n",
    "\n",
    "First, let's load one of the CMIP datasets and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Path to the CMIP data file\ndata_path = \"./data/macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_monthly.nc\"\n\n# Check if file exists\nimport os\nif not os.path.exists(data_path):\n    raise FileNotFoundError(f\"Required CMIP data file not found: {data_path}. Please ensure you're running this notebook from the datacube directory.\")\n\n# Try to initialize the processor\ntry:\n    # Initialize the processor\n    processor = CMIPProcessor(data_path)\nexcept ValueError as e:\n    if \"unable to decode time units\" in str(e) or \"calendar\" in str(e):\n        print(\"Warning: Non-standard calendar detected in the data file.\")\n        print(\"Attempting to fix by manually loading the dataset...\")\n        \n        # Override CMIPProcessor._load_dataset method to handle non-standard calendars\n        import xarray as xr\n        \n        try:\n            # Try to import cftime\n            import cftime\n            print(\"Using cftime for non-standard calendar.\")\n            dataset = xr.open_dataset(data_path, use_cftime=True)\n        except ImportError:\n            print(\"cftime not available. Loading without decoding times.\")\n            dataset = xr.open_dataset(data_path, decode_times=False)\n        \n        # Create a processor and manually set the dataset\n        processor = CMIPProcessor(data_path)\n        processor.dataset = dataset\n    else:\n        # Re-raise if it's a different ValueError\n        raise\n\n# Print dataset information\nprint(\"Dataset Information:\")\nprint(processor.dataset)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Variables\n",
    "\n",
    "Let's look at the variables in the dataset and check their basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Print dataset variables\n",
    "for var_name in processor.dataset.data_vars:\n",
    "    var = processor.dataset[var_name]\n",
    "    print(f\"Variable: {var_name}\")\n",
    "    print(f\"  Dimensions: {var.dims}\")\n",
    "    print(f\"  Shape: {var.shape}\")\n",
    "    print(f\"  Attributes: {var.attrs}\")\n",
    "    print(f\"  Data Statistics:\")\n",
    "    print(f\"    Min: {var.values.min()}\")\n",
    "    print(f\"    Max: {var.values.max()}\")\n",
    "    print(f\"    Mean: {var.values.mean()}\")\n",
    "    print(f\"    Standard Deviation: {var.values.std()}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data Resolution Information\n",
    "\n",
    "Let's determine the native resolution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get spatial resolution\n",
    "lat_res, lon_res = processor.get_spatial_resolution()\n",
    "print(f\"Spatial Resolution: {lat_res}° latitude × {lon_res}° longitude\")\n",
    "\n",
    "# Get temporal resolution\n",
    "time_res = processor.get_temporal_resolution()\n",
    "print(f\"Temporal Resolution: {time_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n",
    "\n",
    "Let's create some visualizations of the data for a specific time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get first variable for plotting\n",
    "var_name = list(processor.dataset.data_vars)[0]\n",
    "var = processor.dataset[var_name]\n",
    "\n",
    "# Plot for the first time point\n",
    "plt.figure(figsize=(12, 8))\n",
    "var.isel(time=0).plot()\n",
    "plt.title(f\"{var_name} at {processor.dataset.time.values[0]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Time Series for a Specific Location\n",
    "\n",
    "Let's extract and plot a time series for a specific location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select a specific location (example coordinates)\n",
    "lat_idx = len(processor.dataset.lat) // 2  # Middle latitude index\n",
    "lon_idx = len(processor.dataset.lon) // 2  # Middle longitude index\n",
    "\n",
    "# Extract lat/lon values\n",
    "lat_val = float(processor.dataset.lat[lat_idx])\n",
    "lon_val = float(processor.dataset.lon[lon_idx])\n",
    "\n",
    "# Extract time series for this location\n",
    "time_series = processor.dataset[var_name].isel(lat=lat_idx, lon=lon_idx)\n",
    "\n",
    "# Plot time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "time_series.plot()\n",
    "plt.title(f\"{var_name} Time Series at Lat: {lat_val:.2f}, Lon: {lon_val:.2f}\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(f\"{var_name} ({time_series.attrs.get('units', '')})\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Spatial Bucketing\n",
    "\n",
    "Now let's demonstrate spatially bucketing the data to a coarser resolution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Apply spatial bucketing (coarsen the resolution)\nlat_bucket_size = lat_res * 2  # Double the native resolution\nlon_bucket_size = lon_res * 2  # Double the native resolution\n\ntry:\n    # Try standard spatial bucketing method\n    spatially_bucketed = processor.bucket_spatial(\n        lat_bucket_size=lat_bucket_size,\n        lon_bucket_size=lon_bucket_size,\n        agg_method=AggregationMethod.MEAN\n    )\nexcept Exception as e:\n    print(f\"Warning: Standard spatial bucketing failed: {e}\")\n    print(\"Using simplified approach with striding...\")\n    \n    # Calculate stride for lat/lon to approximate the requested bucket sizes\n    lat_stride = max(1, int(lat_bucket_size / lat_res))\n    lon_stride = max(1, int(lon_bucket_size / lon_res))\n    \n    print(f\"Using strides: lat={lat_stride}, lon={lon_stride}\")\n    \n    # Create a coarser resolution dataset using slicing\n    spatially_bucketed = processor.dataset.isel(\n        lat=slice(0, None, lat_stride),\n        lon=slice(0, None, lon_stride)\n    )\n    \n    print(f\"Reduced resolution from {processor.dataset.dims} to {spatially_bucketed.dims}\")\n\n# Print information about the bucketed dataset\nprint(f\"Original spatial dimensions: {processor.dataset.dims}\")\nprint(f\"Bucketed spatial dimensions: {spatially_bucketed.dims}\")\n\n# Visualize the spatially bucketed data for the first time point\nplt.figure(figsize=(12, 8))\nvar_name = list(spatially_bucketed.data_vars)[0] # Make sure we have the right var name\nspatially_bucketed[var_name].isel(time=0).plot()\nplt.title(f\"Spatially Bucketed {var_name} (Lat: {lat_bucket_size}°, Lon: {lon_bucket_size}°)\")\nplt.tight_layout()\nplt.show()",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Temporal Bucketing\n",
    "\n",
    "Let's also demonstrate temporally bucketing the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Apply temporal bucketing (e.g., to quarterly data)\ntime_bucket_size = \"3M\"  # Quarterly\n\ntry:\n    # Try standard temporal bucketing\n    temporally_bucketed = processor.bucket_temporal(\n        bucket_size=time_bucket_size,\n        agg_method=AggregationMethod.MEAN\n    )\nexcept Exception as e:\n    print(f\"Warning: Standard temporal bucketing failed: {e}\")\n    print(\"Using simplified approach with striding...\")\n    \n    # For quarterly bucketing, we'll take every 3rd time step\n    stride = 3\n    \n    # Create a coarser temporal resolution dataset using slicing\n    temporally_bucketed = processor.dataset.isel(time=slice(0, None, stride))\n    \n    print(f\"Reduced time resolution from {len(processor.dataset.time)} to {len(temporally_bucketed.time)}\")\n\n# Print information about the bucketed dataset\nprint(f\"Original time dimension: {len(processor.dataset.time)} time points\")\nprint(f\"Bucketed time dimension: {len(temporally_bucketed.time)} time points\")\n\n# Plot the time series for the same location with the temporally bucketed data\ntime_series_bucketed = temporally_bucketed[var_name].isel(lat=lat_idx, lon=lon_idx)\n\nplt.figure(figsize=(12, 6))\ntime_series.plot(label=\"Original\")\ntime_series_bucketed.plot(marker='o', linestyle='-', label=\"Bucketed (Quarterly)\")\nplt.title(f\"{var_name} Time Series - Original vs Temporally Bucketed\")\nplt.xlabel('Time')\nplt.ylabel(f\"{var_name} ({time_series.attrs.get('units', '')})\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Full Datacube\n",
    "\n",
    "Finally, let's create a complete datacube with both spatial and temporal bucketing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Process to datacube with both spatial and temporal bucketing\ndatacube = processor.process_to_datacube(\n    lat_bucket_size=lat_bucket_size,\n    lon_bucket_size=lon_bucket_size,\n    time_bucket_size=time_bucket_size,\n    spatial_agg_method=AggregationMethod.MEAN,\n    temporal_agg_method=AggregationMethod.MEAN\n)\n\n# Print information about the datacube\nprint(\"Datacube Information:\")\nprint(datacube)\n\n# Save the datacube\noutput_path = \"./data/processed/processed_cmip_datacube.nc\"\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nprocessor.save_datacube(datacube, output_path)\nprint(f\"Saved datacube to {output_path}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Aggregation Methods\n",
    "\n",
    "Let's compare how different aggregation methods affect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create datacubes with different aggregation methods\n",
    "agg_methods = [\n",
    "    AggregationMethod.MEAN,\n",
    "    AggregationMethod.MEDIAN,\n",
    "    AggregationMethod.MAX,\n",
    "    AggregationMethod.MIN\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, method in enumerate(agg_methods):\n",
    "    datacube = processor.process_to_datacube(\n",
    "        lat_bucket_size=lat_bucket_size,\n",
    "        lon_bucket_size=lon_bucket_size,\n",
    "        time_bucket_size=None,  # Keep original time resolution for comparison\n",
    "        spatial_agg_method=method\n",
    "    )\n",
    "    \n",
    "    # Plot for the first time point\n",
    "    datacube[var_name].isel(time=0).plot(ax=axes[i])\n",
    "    axes[i].set_title(f\"{method.value.capitalize()} Aggregation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f\"Comparison of Spatial Aggregation Methods for {var_name}\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the CMIP climate data and demonstrated how to:\n",
    "\n",
    "1. Load and examine the dataset structure\n",
    "2. Determine native spatial and temporal resolutions\n",
    "3. Visualize the data spatially and temporally\n",
    "4. Apply spatial and temporal bucketing with different aggregation methods\n",
    "5. Create and save a complete datacube\n",
    "\n",
    "The CMIPProcessor class provides a powerful and flexible way to work with climate data and prepare it for inclusion in larger datacube systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}