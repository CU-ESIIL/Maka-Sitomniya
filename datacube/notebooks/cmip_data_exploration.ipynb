{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CMIP Data Exploration\n",
        "\n",
        "This notebook explores the CMIP climate data files and demonstrates how to work with them using our processing tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add the scripts directory to the path so we can import our modules\n",
        "sys.path.append('../scripts')\n",
        "from cmip_processor import CMIPProcessor, AggregationMethod"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Examine the CMIP Data\n",
        "\n",
        "First, let's load one of the CMIP datasets and examine its structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Path to the CMIP data file\ndata_path = \"./data/macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_monthly.nc\"\n\n# Try to import configuration if available\ntry:\n    # Add parent directory to path to import config\n    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n    import config\n    # Override path with config path if available\n    if hasattr(config, 'CMIP_CONFIG') and hasattr(config, 'DATA_DIR'):\n        model = config.CMIP_CONFIG.get(\"models\", [\"CCSM4\"])[0]\n        variable = config.CMIP_CONFIG.get(\"variables\", [\"huss\"])[0]\n        scenario = config.CMIP_CONFIG.get(\"scenarios\", [\"rcp45\"])[0]\n        years = config.CMIP_CONFIG.get(\"years\", \"2021_2025\")\n        region = config.CMIP_CONFIG.get(\"region\", \"CONUS\")\n        frequency = config.CMIP_CONFIG.get(\"frequency\", \"monthly\")\n        file_name = f\"macav2metdata_{variable}_{model}_r6i1p1_{scenario}_{years}_{region}_{frequency}.nc\"\n        data_path = os.path.join(config.DATA_DIR, file_name)\nexcept ImportError:\n    print(\"Configuration module not found. Using default path.\")\n\n# Check if file exists\nimport os\nif not os.path.exists(data_path):\n    alt_paths = [\n        f\"./data/macav2metdata_huss_CCSM4_r6i1p1_rcp85_2021_2025_CONUS_monthly.nc\",\n        f\"./data/macav2metdata_pr_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_monthly.nc\",\n        f\"./data/macav2metdata_tas*_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_monthly.nc\"\n    ]\n    \n    found_alt = False\n    for alt_path in alt_paths:\n        # Check for wildcard paths\n        import glob\n        if \"*\" in alt_path:\n            matches = glob.glob(alt_path)\n            if matches:\n                data_path = matches[0]\n                found_alt = True\n                print(f\"Using alternative data file: {data_path}\")\n                break\n        elif os.path.exists(alt_path):\n            data_path = alt_path\n            found_alt = True\n            print(f\"Using alternative data file: {data_path}\")\n            break\n    \n    if not found_alt:\n        raise FileNotFoundError(\n            f\"Required CMIP data file not found: {data_path} or alternatives. \"\n            f\"Please ensure you have at least one CMIP data file available in the data directory.\"\n        )\n\n# Try to initialize the processor\ntry:\n    # Initialize the processor\n    processor = CMIPProcessor(data_path)\nexcept ValueError as e:\n    if \"unable to decode time units\" in str(e) or \"calendar\" in str(e):\n        print(\"Warning: Non-standard calendar detected in the data file.\")\n        print(\"Attempting to fix by manually loading the dataset...\")\n        \n        # Override CMIPProcessor._load_dataset method to handle non-standard calendars\n        import xarray as xr\n        \n        try:\n            # Try to import cftime\n            import cftime\n            print(\"Using cftime for non-standard calendar.\")\n            dataset = xr.open_dataset(data_path, use_cftime=True)\n        except ImportError:\n            print(\"cftime not available. Loading without decoding times.\")\n            dataset = xr.open_dataset(data_path, decode_times=False)\n        \n        # Create a processor and manually set the dataset\n        processor = CMIPProcessor(data_path)\n        processor.dataset = dataset\n    else:\n        # Re-raise if it's a different ValueError\n        raise\n\n# Print dataset information\nprint(\"Dataset Information:\")\nprint(processor.dataset)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explore the Variables\n",
        "\n",
        "Let's look at the variables in the dataset and check their basic statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Print dataset variables\n",
        "for var_name in processor.dataset.data_vars:\n",
        "    var = processor.dataset[var_name]\n",
        "    print(f\"Variable: {var_name}\")\n",
        "    print(f\"  Dimensions: {var.dims}\")\n",
        "    print(f\"  Shape: {var.shape}\")\n",
        "    print(f\"  Attributes: {var.attrs}\")\n",
        "    print(f\"  Data Statistics:\")\n",
        "    print(f\"    Min: {var.values.min()}\")\n",
        "    print(f\"    Max: {var.values.max()}\")\n",
        "    print(f\"    Mean: {var.values.mean()}\")\n",
        "    print(f\"    Standard Deviation: {var.values.std()}\")\n",
        "    print(\"\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Data Resolution Information\n",
        "\n",
        "Let's determine the native resolution of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Get spatial resolution\n",
        "lat_res, lon_res = processor.get_spatial_resolution()\n",
        "print(f\"Spatial Resolution: {lat_res}\u00b0 latitude \u00d7 {lon_res}\u00b0 longitude\")\n",
        "\n",
        "# Get temporal resolution\n",
        "time_res = processor.get_temporal_resolution()\n",
        "print(f\"Temporal Resolution: {time_res}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the Data\n",
        "\n",
        "Let's create some visualizations of the data for a specific time point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Get first variable for plotting\nvar_names = list(processor.dataset.data_vars)\nif not var_names:\n    print(\"No variables found in the dataset for plotting.\")\nelse:\n    var_name = var_names[0]\n    var = processor.dataset[var_name]\n    \n    try:\n        # Check that time dimension exists and has at least one value\n        if 'time' in processor.dataset.dims and len(processor.dataset.time) > 0:\n            # Plot for the first time point\n            plt.figure(figsize=(12, 8))\n            var.isel(time=0).plot()\n            plt.title(f\"{var_name} at {processor.dataset.time.values[0]}\")\n            plt.tight_layout()\n            plt.show()\n        else:\n            print(\"Dataset does not have a time dimension or it is empty.\")\n    except Exception as e:\n        print(f\"Error plotting data: {e}\")\n        \n        # Alternative visualization attempt\n        try:\n            plt.figure(figsize=(12, 8))\n            if 'time' in var.dims:\n                # Try first with dropping problematic dimensions\n                var.isel(time=0).plot()\n            else:\n                # Try plotting without time dimension\n                var.plot()\n            plt.title(f\"{var_name}\")\n            plt.tight_layout()\n            plt.show()\n        except Exception as nested_e:\n            print(f\"Alternative plotting also failed: {nested_e}\")\n            print(\"Cannot create visualization.\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Time Series for a Specific Location\n",
        "\n",
        "Let's extract and plot a time series for a specific location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Check if the dataset has required dimensions\nif 'lat' not in processor.dataset.dims or 'lon' not in processor.dataset.dims:\n    print(\"Dataset does not have lat and/or lon dimensions required for location-based time series.\")\nelif 'time' not in processor.dataset.dims:\n    print(\"Dataset does not have a time dimension required for time series analysis.\")\nelif len(processor.dataset.time) <= 1:\n    print(\"Dataset has only one time point. Cannot create time series.\")\nelse:\n    try:\n        # Find the variable for time series plotting\n        var_names = list(processor.dataset.data_vars)\n        if not var_names:\n            print(\"No variables found in the dataset for time series plotting.\")\n        else:\n            var_name = var_names[0]\n            var = processor.dataset[var_name]\n            \n            # Select a specific location (example coordinates)\n            lat_idx = len(processor.dataset.lat) // 2  # Middle latitude index\n            lon_idx = len(processor.dataset.lon) // 2  # Middle longitude index\n\n            # Extract lat/lon values\n            lat_val = float(processor.dataset.lat[lat_idx])\n            lon_val = float(processor.dataset.lon[lon_idx])\n\n            # Extract time series for this location\n            time_series = processor.dataset[var_name].isel(lat=lat_idx, lon=lon_idx)\n\n            # Plot time series\n            plt.figure(figsize=(12, 6))\n            time_series.plot()\n            plt.title(f\"{var_name} Time Series at Lat: {lat_val:.2f}, Lon: {lon_val:.2f}\")\n            plt.xlabel('Time')\n            plt.ylabel(f\"{var_name} ({time_series.attrs.get('units', '')})\")\n            plt.grid(True)\n            plt.tight_layout()\n            plt.show()\n    except Exception as e:\n        print(f\"Error creating time series plot: {e}\")\n        \n        # Try an alternative approach with simplified data extraction\n        try:\n            var_name = list(processor.dataset.data_vars)[0]\n            # Find middle points using different methods\n            lat_dim = next((d for d in processor.dataset.dims if 'lat' in d.lower()), None)\n            lon_dim = next((d for d in processor.dataset.dims if 'lon' in d.lower()), None)\n            \n            if lat_dim and lon_dim:\n                lat_idx = len(processor.dataset[lat_dim]) // 2\n                lon_idx = len(processor.dataset[lon_dim]) // 2\n                \n                # Create selection dict\n                selection = {'time': slice(None)}  # All time points\n                selection[lat_dim] = lat_idx\n                selection[lon_dim] = lon_idx\n                \n                # Extract time series\n                time_series = processor.dataset[var_name].isel(**selection)\n                \n                # Plot time series\n                plt.figure(figsize=(12, 6))\n                time_series.plot()\n                plt.title(f\"{var_name} Time Series at center point\")\n                plt.xlabel('Time')\n                plt.ylabel(f\"{var_name}\")\n                plt.grid(True)\n                plt.tight_layout()\n                plt.show()\n            else:\n                print(\"Could not find latitude and longitude dimensions.\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Spatial Bucketing\n",
        "\n",
        "Now let's demonstrate spatially bucketing the data to a coarser resolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Check if we can get spatial resolution\ntry:\n    # Apply spatial bucketing (coarsen the resolution)\n    lat_res, lon_res = processor.get_spatial_resolution()\n    lat_bucket_size = lat_res * 2  # Double the native resolution\n    lon_bucket_size = lon_res * 2  # Double the native resolution\n    \n    try:\n        # Try standard spatial bucketing method\n        spatially_bucketed = processor.bucket_spatial(\n            lat_bucket_size=lat_bucket_size,\n            lon_bucket_size=lon_bucket_size,\n            agg_method=AggregationMethod.MEAN\n        )\n    except Exception as e:\n        print(f\"Warning: Standard spatial bucketing failed: {e}\")\n        print(\"Using simplified approach with striding...\")\n        \n        # Calculate stride for lat/lon to approximate the requested bucket sizes\n        lat_stride = max(1, int(lat_bucket_size / lat_res))\n        lon_stride = max(1, int(lon_bucket_size / lon_res))\n        \n        print(f\"Using strides: lat={lat_stride}, lon={lon_stride}\")\n        \n        # Find latitude and longitude dimension names\n        lat_dim = next((d for d in processor.dataset.dims if 'lat' in d.lower()), 'lat')\n        lon_dim = next((d for d in processor.dataset.dims if 'lon' in d.lower()), 'lon')\n        \n        # Create a coarser resolution dataset using slicing\n        selection = {}\n        selection[lat_dim] = slice(0, None, lat_stride)\n        selection[lon_dim] = slice(0, None, lon_stride)\n        \n        spatially_bucketed = processor.dataset.isel(**selection)\n        \n        print(f\"Reduced resolution from {processor.dataset.dims} to {spatially_bucketed.dims}\")\n\n    # Print information about the bucketed dataset\n    print(f\"Original spatial dimensions: {processor.dataset.dims}\")\n    print(f\"Bucketed spatial dimensions: {spatially_bucketed.dims}\")\n\n    # Visualize the spatially bucketed data for the first time point\n    var_names = list(spatially_bucketed.data_vars)\n    if var_names:\n        var_name = var_names[0] # Make sure we have the right var name\n        try:\n            plt.figure(figsize=(12, 8))\n            if 'time' in spatially_bucketed.dims and len(spatially_bucketed.time) > 0:\n                spatially_bucketed[var_name].isel(time=0).plot()\n                plt.title(f\"Spatially Bucketed {var_name} (Lat: {lat_bucket_size}\u00b0, Lon: {lon_bucket_size}\u00b0)\")\n            else:\n                spatially_bucketed[var_name].plot()\n                plt.title(f\"Spatially Bucketed {var_name} (Lat: {lat_bucket_size}\u00b0, Lon: {lon_bucket_size}\u00b0)\")\n            plt.tight_layout()\n            plt.show()\n        except Exception as e:\n            print(f\"Error visualizing bucketed data: {e}\")\n    else:\n        print(\"No variables found in the bucketed dataset for visualization.\")\nexcept Exception as e:\n    print(f\"Error in spatial bucketing process: {e}\")\n    print(\"Cannot perform spatial bucketing on this dataset.\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Temporal Bucketing\n",
        "\n",
        "Let's also demonstrate temporally bucketing the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Check if time dimension exists\nif 'time' not in processor.dataset.dims:\n    print(\"Dataset does not have a time dimension. Cannot perform temporal bucketing.\")\nelif len(processor.dataset.time) <= 1:\n    print(\"Dataset has only one time point. Temporal bucketing not meaningful.\")\nelse:\n    try:\n        # Apply temporal bucketing (e.g., to quarterly data)\n        time_bucket_size = \"3M\"  # Quarterly\n\n        try:\n            # Try standard temporal bucketing\n            temporally_bucketed = processor.bucket_temporal(\n                bucket_size=time_bucket_size,\n                agg_method=AggregationMethod.MEAN\n            )\n        except Exception as e:\n            print(f\"Warning: Standard temporal bucketing failed: {e}\")\n            print(\"Using simplified approach with striding...\")\n            \n            # For quarterly bucketing, we'll take every 3rd time step\n            stride = 3\n            \n            # Create a coarser temporal resolution dataset using slicing\n            temporally_bucketed = processor.dataset.isel(time=slice(0, None, stride))\n            \n            print(f\"Reduced time resolution from {len(processor.dataset.time)} to {len(temporally_bucketed.time)}\")\n\n        # Print information about the bucketed dataset\n        print(f\"Original time dimension: {len(processor.dataset.time)} time points\")\n        print(f\"Bucketed time dimension: {len(temporally_bucketed.time)} time points\")\n\n        # Get variable for plotting\n        var_names = list(processor.dataset.data_vars)\n        if var_names:\n            var_name = var_names[0]\n            \n            try:\n                # Check if we have spatial dimensions\n                if 'lat' in processor.dataset.dims and 'lon' in processor.dataset.dims:\n                    # Find a suitable location for time series comparison\n                    lat_idx = len(processor.dataset.lat) // 2  # Middle latitude index\n                    lon_idx = len(processor.dataset.lon) // 2  # Middle longitude index\n                    \n                    # Get time series for original and bucketed data\n                    time_series = processor.dataset[var_name].isel(lat=lat_idx, lon=lon_idx)\n                    time_series_bucketed = temporally_bucketed[var_name].isel(lat=lat_idx, lon=lon_idx)\n                    \n                    # Plot comparison\n                    plt.figure(figsize=(12, 6))\n                    time_series.plot(label=\"Original\")\n                    time_series_bucketed.plot(marker='o', linestyle='-', label=\"Bucketed (Quarterly)\")\n                    plt.title(f\"{var_name} Time Series - Original vs Temporally Bucketed\")\n                    plt.xlabel('Time')\n                    plt.ylabel(f\"{var_name} ({time_series.attrs.get('units', '')})\")\n                    plt.grid(True)\n                    plt.legend()\n                    plt.tight_layout()\n                    plt.show()\n                else:\n                    print(\"Dataset missing lat/lon dimensions. Cannot create location-based time series.\")\n            except Exception as e:\n                print(f\"Error plotting time series comparison: {e}\")\n                \n                # Try alternative dimension names\n                try:\n                    lat_dim = next((d for d in processor.dataset.dims if 'lat' in d.lower()), None)\n                    lon_dim = next((d for d in processor.dataset.dims if 'lon' in d.lower()), None)\n                    \n                    if lat_dim and lon_dim:\n                        lat_idx = len(processor.dataset[lat_dim]) // 2\n                        lon_idx = len(processor.dataset[lon_dim]) // 2\n                        \n                        # Create selection dict\n                        selection = {}\n                        selection[lat_dim] = lat_idx\n                        selection[lon_dim] = lon_idx\n                        \n                        # Get time series for original and bucketed data\n                        time_series = processor.dataset[var_name].isel(**selection)\n                        time_series_bucketed = temporally_bucketed[var_name].isel(**selection)\n                        \n                        # Plot comparison\n                        plt.figure(figsize=(12, 6))\n                        time_series.plot(label=\"Original\")\n                        time_series_bucketed.plot(marker='o', linestyle='-', label=\"Bucketed (Quarterly)\")\n                        plt.title(f\"{var_name} Time Series - Original vs Temporally Bucketed\")\n                        plt.xlabel('Time')\n                        plt.ylabel(f\"{var_name}\")\n                        plt.grid(True)\n                        plt.legend()\n                        plt.tight_layout()\n                        plt.show()\n                    else:\n                        print(\"Could not find suitable spatial dimensions for time series.\")\n                except Exception as nested_e:\n                    print(f\"Alternative plotting also failed: {nested_e}\")\n        else:\n            print(\"No variables found in the dataset for visualization.\")\n    except Exception as e:\n        print(f\"Error in temporal bucketing process: {e}\")\n        print(\"Cannot perform temporal bucketing on this dataset.\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Full Datacube\n",
        "\n",
        "Finally, let's create a complete datacube with both spatial and temporal bucketing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Check if we have the required variables defined\nrequired_vars = ['lat_bucket_size', 'lon_bucket_size', 'time_bucket_size']\nmissing_vars = [var for var in required_vars if var not in locals()]\n\nif missing_vars:\n    # Define default values if not already defined\n    if 'lat_bucket_size' not in locals() or 'lon_bucket_size' not in locals():\n        try:\n            lat_res, lon_res = processor.get_spatial_resolution()\n            lat_bucket_size = lat_res * 2\n            lon_bucket_size = lon_res * 2\n            print(f\"Using default spatial bucket sizes: lat={lat_bucket_size}\u00b0, lon={lon_bucket_size}\u00b0\")\n        except Exception as e:\n            print(f\"Error getting spatial resolution: {e}\")\n            lat_bucket_size = 0.5\n            lon_bucket_size = 0.5\n            print(f\"Using fallback spatial bucket sizes: lat={lat_bucket_size}\u00b0, lon={lon_bucket_size}\u00b0\")\n    \n    if 'time_bucket_size' not in locals():\n        time_bucket_size = \"3M\"  # Quarterly\n        print(f\"Using default time bucket size: {time_bucket_size}\")\n\ntry:\n    # Process to datacube with both spatial and temporal bucketing\n    datacube = processor.process_to_datacube(\n        lat_bucket_size=lat_bucket_size,\n        lon_bucket_size=lon_bucket_size,\n        time_bucket_size=time_bucket_size,\n        spatial_agg_method=AggregationMethod.MEAN,\n        temporal_agg_method=AggregationMethod.MEAN\n    )\n\n    # Print information about the datacube\n    print(\"Datacube Information:\")\n    print(datacube)\n\n    # Create output directory if it doesn't exist\n    output_path = \"./data/processed/processed_cmip_datacube.nc\"\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    \n    # Save the datacube\n    processor.save_datacube(datacube, output_path)\n    print(f\"Saved datacube to {output_path}\")\nexcept Exception as e:\n    print(f\"Error creating full datacube: {e}\")\n    print(\"Attempting simplified approach...\")\n    \n    try:\n        # Use simplified striding approach\n        lat_dim = next((d for d in processor.dataset.dims if 'lat' in d.lower()), 'lat')\n        lon_dim = next((d for d in processor.dataset.dims if 'lon' in d.lower()), 'lon')\n        \n        # Calculate stride values\n        lat_res, lon_res = processor.get_spatial_resolution()\n        lat_stride = max(1, int(lat_bucket_size / lat_res))\n        lon_stride = max(1, int(lon_bucket_size / lon_res))\n        time_stride = 3 if time_bucket_size == \"3M\" else 1\n        \n        # Create selection dict\n        selection = {}\n        if 'time' in processor.dataset.dims:\n            selection['time'] = slice(0, None, time_stride)\n        selection[lat_dim] = slice(0, None, lat_stride)\n        selection[lon_dim] = slice(0, None, lon_stride)\n        \n        # Create simplified datacube\n        simplified_datacube = processor.dataset.isel(**selection)\n        \n        print(\"Created simplified datacube:\")\n        print(simplified_datacube)\n        \n        # Save the simplified datacube\n        output_path = \"./data/processed/simplified_cmip_datacube.nc\"\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        simplified_datacube.to_netcdf(output_path)\n        print(f\"Saved simplified datacube to {output_path}\")\n    except Exception as nested_e:\n        print(f\"Simplified approach also failed: {nested_e}\")\n        print(\"Could not create datacube.\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Different Aggregation Methods\n",
        "\n",
        "Let's compare how different aggregation methods affect the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create datacubes with different aggregation methods\n",
        "agg_methods = [\n",
        "    AggregationMethod.MEAN,\n",
        "    AggregationMethod.MEDIAN,\n",
        "    AggregationMethod.MAX,\n",
        "    AggregationMethod.MIN\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, method in enumerate(agg_methods):\n",
        "    datacube = processor.process_to_datacube(\n",
        "        lat_bucket_size=lat_bucket_size,\n",
        "        lon_bucket_size=lon_bucket_size,\n",
        "        time_bucket_size=None,  # Keep original time resolution for comparison\n",
        "        spatial_agg_method=method\n",
        "    )\n",
        "    \n",
        "    # Plot for the first time point\n",
        "    datacube[var_name].isel(time=0).plot(ax=axes[i])\n",
        "    axes[i].set_title(f\"{method.value.capitalize()} Aggregation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(f\"Comparison of Spatial Aggregation Methods for {var_name}\", y=1.02, fontsize=16)\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've explored the CMIP climate data and demonstrated how to:\n",
        "\n",
        "1. Load and examine the dataset structure\n",
        "2. Determine native spatial and temporal resolutions\n",
        "3. Visualize the data spatially and temporally\n",
        "4. Apply spatial and temporal bucketing with different aggregation methods\n",
        "5. Create and save a complete datacube\n",
        "\n",
        "The CMIPProcessor class provides a powerful and flexible way to work with climate data and prepare it for inclusion in larger datacube systems."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}